{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tm6qpMgBo4iL",
    "outputId": "e12a918b-8bb4-4bb7-d4ba-216a4246247a"
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing\n",
    "from worker import worker\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLUDbHD6tUIA"
   },
   "source": [
    "## 1. 데이터 가공\n",
    "userId_problemId.csv 파일 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9S15_wWOqlrO",
    "outputId": "9e285579-6534-4b81-e2ac-a086686c6633"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>problemId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sos0911</td>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    userId  problemId\n",
       "0  sos0911       1000\n",
       "1  sos0911       1001\n",
       "2  sos0911       1002\n",
       "3  sos0911       1003\n",
       "4  sos0911       1005"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:,['userId', 'problemId']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDzVw2frtXET"
   },
   "source": [
    "### 1) userId remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RtTOGWMUtQx4"
   },
   "outputs": [],
   "source": [
    "def remap_user():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.DataFrame({\"user_id\": [], \"remap_id\": []}).astype({'remap_id':'int'})\n",
    "    unique_user = user_problem_df['userId'].unique().tolist()\n",
    "    for i in range(len(unique_user)):\n",
    "        new_user_df = pd.DataFrame({\"user_id\": [unique_user[i]], \"remap_id\": [i]})\n",
    "        user_df = pd.concat((user_df, new_user_df))\n",
    "        \n",
    "    user_df.to_csv(\"./data/user_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNoHgbOBtY6l"
   },
   "source": [
    "### 2) problemId remapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kO3kEm6-tYTf"
   },
   "outputs": [],
   "source": [
    "def remap_problem():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    problem_df = pd.DataFrame({\"problem_id\": [], \"remap_id\": []}).astype({'remap_id':'int'})\n",
    "    unique_sorted_problem = sorted(user_problem_df['problemId'].unique().tolist())\n",
    "    for i in range(len(unique_sorted_problem)):\n",
    "        new_problem_df = pd.DataFrame({\"problem_id\": [unique_sorted_problem[i]], \"remap_id\": [i]})\n",
    "        problem_df = pd.concat((problem_df, new_problem_df))\n",
    "    \n",
    "    problem_df.to_csv(\"./data/problem_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvw2dsc6tcqQ"
   },
   "source": [
    "### 3) Convert userId, problemId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "oK5oz3matcO4"
   },
   "outputs": [],
   "source": [
    "def remap_user_problem():\n",
    "    user_problem_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.read_csv(\"./data/user_list.csv\")\n",
    "    problem_df = pd.read_csv(\"./data/problem_list.csv\")\n",
    "\n",
    "    manager = multiprocessing.Manager()\n",
    "    user_problem_remap_df = pd.DataFrame({'userId': [], 'problemId': []}).astype('int')\n",
    "\n",
    "    for i in range(0, user_problem_df['userId'].size, 10000):\n",
    "        clear_output(wait=True)\n",
    "        print('Loading: [{}]'.format('-' * (i // 10000) + '>' + '-' * (83 - i // 10000)))\n",
    "        \n",
    "        return_dict = manager.dict()\n",
    "        jobs = []\n",
    "        \n",
    "        for j in range(4):\n",
    "            p = multiprocessing.Process(target=worker, args=(j, i + 2500 * j, user_df, problem_df, user_problem_df[i + 2500 * j:i + min(user_problem_df['userId'].size, 2500*(j+1))], return_dict))\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            if i + 2500 * (j+1) >= user_problem_df['userId'].size: break\n",
    "        for proc in jobs:\n",
    "            proc.join()\n",
    "            proc.close()\n",
    "        for j in range(len(return_dict.keys())):\n",
    "            user_problem_remap_df = pd.concat((user_problem_remap_df, return_dict[j]))\n",
    "    user_problem_remap_df.to_csv(\"./data/userId_problemId_remap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vytaxrOotiJe",
    "outputId": "44015654-ad88-4bc8-96df-11ecf5a4cdeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_list.csv file already exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/user_list.csv'):\n",
    "    print('user_list.csv file already exist')\n",
    "else: \n",
    "    remap_user()\n",
    "    print('user_list file saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ieHaZKROtlTm",
    "outputId": "6c196f54-46e6-451f-cf59-e6945a5fb761"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem_list.csv file already exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/problem_list.csv'):\n",
    "    print('problem_list.csv file already exist')\n",
    "else:\n",
    "    remap_problem()\n",
    "    print('problem_list file saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKCMiJsAtq6o"
   },
   "source": [
    "multiprocessing을 사용하여 30분 이상 걸리던 작업을 13분으로 단축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPGmdhM0tm-k",
    "outputId": "bf671e9d-748e-4936-f843-3876db7275f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId_problemId_remap.csv file already exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('./data/userId_problemId_remap.csv'):\n",
    "    print('userId_problemId_remap.csv file already exist')\n",
    "else:\n",
    "    remap_user_problem()\n",
    "    print('userId_problemId_remap file saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxQdJFTkttde"
   },
   "source": [
    "### train data, test data 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5QHgf8I2tpCK"
   },
   "outputs": [],
   "source": [
    "def making_data():\n",
    "\n",
    "    if os.path.isfile('./data/train.txt') and os.path.isfile('./data/test.txt'):\n",
    "        print('train.txt test.txt file already exist')\n",
    "        return\n",
    "\n",
    "    user_problem_remap_df = pd.read_csv(\"./data/userId_problemId_remap.csv\").loc[:, ['userId', 'problemId']]\n",
    "    train_data_df = pd.DataFrame({'userId': [], 'problemId': []}, dtype=int)\n",
    "    test_data_df = pd.DataFrame({'userId': [], 'problemId': []}, dtype=int)\n",
    "    \n",
    "    for user_id in user_problem_remap_df['userId'].unique():\n",
    "        problem_list = user_problem_remap_df[user_problem_remap_df['userId'] == user_id]['problemId'].tolist()\n",
    "        train_problem_list = random.sample(problem_list, int(len(problem_list) * 0.8))\n",
    "        test_problem_list = list(set(problem_list) - set(train_problem_list))\n",
    "        \n",
    "        new_train_data_df = pd.DataFrame({'userId': [user_id], 'problemId': [train_problem_list]})\n",
    "        new_test_data_df = pd.DataFrame({'userId': [user_id], 'problemId': [test_problem_list]})\n",
    "        \n",
    "        train_data_df = pd.concat((train_data_df, new_train_data_df))\n",
    "        test_data_df = pd.concat((test_data_df, new_test_data_df))\n",
    "    \n",
    "    with open(\"./data/train.txt\", \"w\") as f:\n",
    "        for user_id in sorted(train_data_df['userId'].tolist()):\n",
    "            problem_list = [*map(int, train_data_df[train_data_df['userId'] == user_id]['problemId'].tolist()[0])]\n",
    "            f.write(str(user_id) + ' ' + ' '.join(map(str, problem_list)) + '\\n')\n",
    "    \n",
    "    with open(\"./data/test.txt\", \"w\") as f:\n",
    "        for user_id in sorted(test_data_df['userId'].tolist()):\n",
    "            problem_list = [*map(int, test_data_df[test_data_df['userId'] == user_id]['problemId'].tolist()[0])]\n",
    "            f.write(str(user_id) + ' ' + ' '.join(map(str, problem_list)) + '\\n')\n",
    "        \n",
    "    print('train.txt, test.txt saved successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Dx9jR-0tvO7",
    "outputId": "9ed38500-9a1a-4c90-858c-a01c8b974cbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.txt test.txt file already exist\n"
     ]
    }
   ],
   "source": [
    "making_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI7AIfcCtxbK"
   },
   "source": [
    "# NGCF 모델 관련 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2Xq4bIRtyFx"
   },
   "source": [
    "### 전역변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ENuY5LnMtwJr"
   },
   "outputs": [],
   "source": [
    "train_items, test_set = {}, {}\n",
    "matrix = None\n",
    "exist_users = []\n",
    "global_epoch_value = 0\n",
    "result_arr = []\n",
    "\n",
    "n_users, n_items, n_train, n_test = 0, 0, 0, 0\n",
    "total_epoch = 2000\n",
    "embed_size = 64\n",
    "batch_size = 1024\n",
    "layer_size = [64, 64, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z03bCWbit0wb"
   },
   "source": [
    "### NGCF 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "F_TVKh_xtz0t"
   },
   "outputs": [],
   "source": [
    "class NGCF(nn.Module):\n",
    "    def __init__(self, n_user, n_item, norm_adj, emb_size, batch_size, layer_size):\n",
    "        super(NGCF, self).__init__()\n",
    "        self.n_user = n_user\n",
    "        self.n_item = n_item\n",
    "        self.norm_adj = norm_adj.cuda()\n",
    "        self.emb_size = emb_size\n",
    "        self.batch_size = batch_size\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "        self.embedding_dict, self.weight_dict = self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        embedding_dict = nn.ParameterDict({\n",
    "            'user_emb': nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.n_user, self.emb_size))),\n",
    "            'item_emb': nn.Parameter(nn.init.xavier_uniform_(torch.empty(self.n_item, self.emb_size)))\n",
    "        })\n",
    "\n",
    "        weight_dict = nn.ParameterDict()\n",
    "        layers = [self.emb_size] + self.layer_size\n",
    "        for i in range(len(self.layer_size)):\n",
    "            weight_dict['W_gc_%d' % i] = nn.Parameter(nn.init.xavier_uniform_(torch.empty(layers[i], layers[i + 1])))\n",
    "            weight_dict['W_bi_%d' % i] = nn.Parameter(nn.init.xavier_uniform_(torch.empty(layers[i], layers[i + 1])))\n",
    "\n",
    "        return embedding_dict, weight_dict\n",
    "\n",
    "    def rating(self, u_g_embeddings, pos_i_g_embeddings):\n",
    "        return torch.matmul(u_g_embeddings, pos_i_g_embeddings.t())\n",
    "\n",
    "    def forward(self, users, pos_items, neg_items):\n",
    "        ego_embeddings = torch.cat([self.embedding_dict['user_emb'], self.embedding_dict['item_emb']], 0)\n",
    "        all_embeddings = [ego_embeddings]\n",
    "\n",
    "        for i in range(len(self.layer_size)):\n",
    "            side_embeddings = torch.mm(self.norm_adj, ego_embeddings)\n",
    "\n",
    "            sum_embeddings = torch.matmul(ego_embeddings, self.weight_dict['W_gc_%d' % i])\n",
    "            bi_embeddings = torch.mul(ego_embeddings, side_embeddings)\n",
    "            bi_embeddings = torch.matmul(bi_embeddings, self.weight_dict['W_gc_%d' % i])\n",
    "\n",
    "            ego_embeddings = nn.LeakyReLU(negative_slope=0.2)(sum_embeddings + bi_embeddings)\n",
    "            norm_embeddings = F.normalize(ego_embeddings, p=2, dim=1)\n",
    "            all_embeddings += [norm_embeddings]\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, 1)\n",
    "        u_g_embeddings = all_embeddings[:self.n_user, :]\n",
    "        i_g_embeddings = all_embeddings[self.n_user:, :]\n",
    "\n",
    "        u_g_embeddings = u_g_embeddings[users, :]\n",
    "        pos_i_g_embeddings = i_g_embeddings[pos_items, :]\n",
    "        neg_i_g_embeddings = i_g_embeddings[neg_items, :]\n",
    "\n",
    "        return u_g_embeddings, pos_i_g_embeddings, neg_i_g_embeddings\n",
    "\n",
    "    def BPR_Loss(self, users, pos_items, neg_items):\n",
    "        pos_scores = torch.sum(torch.mul(users, pos_items), axis=1)\n",
    "        neg_scores = torch.sum(torch.mul(users, neg_items), axis=1)\n",
    "\n",
    "        maxi = nn.LogSigmoid()(pos_scores - neg_scores)\n",
    "\n",
    "        mf_loss = -1 * torch.mean(maxi)\n",
    "\n",
    "        regularizer = (torch.norm(users) ** 2 + torch.norm(pos_items) ** 2 + torch.norm(neg_items) ** 2) / 2\n",
    "        decay = 1e-5\n",
    "        emb_loss = decay * regularizer / self.batch_size\n",
    "\n",
    "        return mf_loss + emb_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_v8jmU4t2Ye"
   },
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7CAdhgdxt1-d"
   },
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    global n_users, n_items, n_train, n_test, matrix\n",
    "\n",
    "    train_file = './data/train.txt'\n",
    "    test_file = './data/test.txt'\n",
    "\n",
    "    with open(train_file) as f:\n",
    "        for line in f.readlines():\n",
    "            x = line.strip().split()\n",
    "\n",
    "            user_id = int(x[0])\n",
    "            exist_users.append(user_id)\n",
    "            n_users = max(n_users, user_id)\n",
    "\n",
    "            items = [*map(int, x[1:])]\n",
    "            n_items = max(n_items, max(items))\n",
    "\n",
    "            n_train += len(items)\n",
    "\n",
    "    with open(test_file) as f:\n",
    "        for line in f.readlines():\n",
    "            x = line.strip().split()\n",
    "\n",
    "            items = [*map(int, x[1:])]\n",
    "            n_items = max(n_items, max(items))\n",
    "            n_test += len(items)\n",
    "\n",
    "    n_users += 1\n",
    "    n_items += 1\n",
    "\n",
    "    matrix = torch.zeros((n_users, n_items))\n",
    "\n",
    "    with open(train_file) as f_train:\n",
    "        with open(test_file) as f_test:\n",
    "            for line in f_train.readlines():\n",
    "                x = line.strip().split()\n",
    "                items = [*map(int, x)]\n",
    "                user_id, t_items = items[0], items[1:]\n",
    "                for item in t_items:\n",
    "                    matrix[user_id, item] = 1\n",
    "                train_items[user_id] = t_items\n",
    "            for line in f_test.readlines():\n",
    "                x = line.strip().split()\n",
    "                items = [*map(int, x)]\n",
    "                user_id, t_items = items[0], items[1:]\n",
    "                test_set[user_id] = t_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOZh7BwWuBap"
   },
   "source": [
    "### 학습할 때 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sGqT_VzyuAKS"
   },
   "outputs": [],
   "source": [
    "def sample():\n",
    "    if batch_size <= n_users:\n",
    "        users = random.sample(exist_users, batch_size)\n",
    "    else:\n",
    "        users = [random.choice(exist_users) for i in range(batch_size)]\n",
    "\n",
    "    pos_items, neg_items = [], []\n",
    "    for user in users:\n",
    "        pos_item_list = train_items[user]\n",
    "        pos_batch = pos_item_list[np.random.randint(0, len(pos_item_list))]\n",
    "        pos_items += [pos_batch]\n",
    "\n",
    "        while 1:\n",
    "            neg_item_list = train_items[user]\n",
    "            neg_id = np.random.randint(0, n_items)\n",
    "            if neg_id not in train_items[user] and neg_id not in neg_item_list:\n",
    "                neg_items.append(neg_id)\n",
    "                break\n",
    "    return users, pos_items, neg_items\n",
    "\n",
    "\n",
    "def get_norm_adj():\n",
    "    adj_mat = torch.zeros([n_users + n_items, n_users + n_items])\n",
    "    adj_mat[:n_users, n_users:] = matrix\n",
    "    adj_mat[n_users:, :n_users] = matrix.T\n",
    "    rowsum = np.array(adj_mat.sum(1))\n",
    "    d_inv = rowsum.copy()\n",
    "    for i in range(rowsum.size):\n",
    "        if d_inv[i] != 0:\n",
    "            d_inv[i] = 1 / d_inv[i]\n",
    "    d_mat_inv = np.diag(d_inv)\n",
    "\n",
    "    return torch.from_numpy(d_mat_inv.dot(adj_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxZOHFZuuEcu"
   },
   "source": [
    "### 평가할 때 필요한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wH7jCTGouDLH"
   },
   "outputs": [],
   "source": [
    "def ranklist_by_heapq(user_pos_test, test_items, rating, Ks):\n",
    "\n",
    "    item_score = {}\n",
    "    for i in test_items:\n",
    "        item_score[i] = rating[i]\n",
    "    K_max = Ks\n",
    "    K_max_item_score = heapq.nlargest(K_max, item_score, key=item_score.get)\n",
    "\n",
    "    if (global_epoch_value + 1) % 100 == 0:\n",
    "        path = f'./data/rank_{global_epoch_value+1}.txt'\n",
    "        f = open(path, 'a')\n",
    "        f.write(' '.join(map(str, K_max_item_score)) + '\\n')\n",
    "        f.close()\n",
    "    r = []\n",
    "    for val in K_max_item_score:\n",
    "        if val in user_pos_test:\n",
    "            r += [1]\n",
    "        else:\n",
    "            r += [0]\n",
    "    return r\n",
    "\n",
    "\n",
    "def get_performance(r, Ks):\n",
    "    return np.mean(np.asarray(r)[:Ks])\n",
    "\n",
    "\n",
    "def test_one_user(x, y):\n",
    "    rating = x\n",
    "    user = y\n",
    "    if len(train_items[user]) == 0:\n",
    "        training_items = []\n",
    "    else:\n",
    "        training_items = train_items[user]\n",
    "\n",
    "    user_pos_test = test_set[user]\n",
    "    all_items = set(range(n_items))\n",
    "    test_items = list(all_items - set(training_items))\n",
    "    r = ranklist_by_heapq(user_pos_test, test_items, rating, 2000)\n",
    "\n",
    "    return get_performance(r, 2000)\n",
    "\n",
    "\n",
    "def test(model, users_to_test):\n",
    "    result = 0\n",
    "    u_batch_size = batch_size * 2\n",
    "\n",
    "    test_users = users_to_test\n",
    "    n_test_users = len(test_users)\n",
    "    n_users_batchs = n_test_users // u_batch_size + 1\n",
    "\n",
    "    for u_batch_id in range(n_users_batchs):\n",
    "        start = u_batch_id * u_batch_size\n",
    "        end = (u_batch_id + 1) * u_batch_size\n",
    "\n",
    "        user_batch = test_users[start:end]\n",
    "        item_batch = range(n_items)\n",
    "        u_g_embedding, pos_i_g_embedding, _ = model(user_batch, item_batch, [])\n",
    "        rate_batch = model.rating(u_g_embedding, pos_i_g_embedding).detach().cpu()\n",
    "\n",
    "        for i in range(len(user_batch)):\n",
    "            result += test_one_user(rate_batch.numpy()[i], user_batch[i])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hgs5CZ3EuG3l"
   },
   "source": [
    "### 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zfUo9DFouFre"
   },
   "outputs": [],
   "source": [
    "def get_result(epoch_value = 2000, flag = True):\n",
    "    '''\n",
    "    :param epoch_value: epoch 값\n",
    "    :param flag: True면 이미 저장된 값을 return, False면 새로 학습시켜서 return\n",
    "    :return: 각 유저에게 유사도가 제일 높은 순서로 문제 번호를 반환하는 2차원 리스트\n",
    "    '''\n",
    "    global n_users, n_items, n_train, n_test, total_epoch, embed_size, batch_size, global_epoch_value\n",
    "\n",
    "    result_arr = []\n",
    "\n",
    "    if flag:\n",
    "        if os.path.isfile(f'./data/rank.txt'):\n",
    "            print('file exist')\n",
    "            f = open(f'./data/rank.txt', 'r')\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                result_arr.append([*line.strip().split()])\n",
    "            f.close()\n",
    "            return result_arr\n",
    "        else:\n",
    "            print('file does not exist')\n",
    "\n",
    "    n_users, n_items, n_train, n_test = 0, 0, 0, 0\n",
    "    data_load()\n",
    "    norm_adj = get_norm_adj()\n",
    "    total_epoch = epoch_value\n",
    "    embed_size = 64\n",
    "    batch_size = 1024\n",
    "    layer_size = [64, 64, 64]\n",
    "\n",
    "    max_model_value = 0\n",
    "    for i in range(100, 2001, 100):\n",
    "        if os.path.isfile(f'./model/NGCF_model_{i}.pkl'):\n",
    "            max_model_value = i\n",
    "    if max_model_value != 0:\n",
    "        print(f'file exist NGCF_model_{max_model_value}')\n",
    "        model = joblib.load(f'./model/NGCF_model_{max_model_value}.pkl')\n",
    "    else: model = NGCF(n_users, n_items, norm_adj, embed_size, batch_size, layer_size).cuda()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(total_epoch):\n",
    "        global_epoch_value = epoch\n",
    "        time1 = time.time()\n",
    "\n",
    "        loss = 0\n",
    "        n_batch = n_train // batch_size + 1\n",
    "        for idx in range(n_batch):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            users, pos_items, neg_items = sample()\n",
    "            u_g_embedding, pos_i_g_embedding, neg_i_g_embedding = model(users, pos_items, neg_items)\n",
    "            batch_loss = model.BPR_Loss(u_g_embedding, pos_i_g_embedding, neg_i_g_embedding)\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss += batch_loss\n",
    "\n",
    "        time2 = time.time()\n",
    "        print(f'Epoch: {epoch}, loss: {loss}, time: {int(time2 - time1)}')\n",
    "\n",
    "        users_to_test = list(test_set.keys())\n",
    "        ret = test(model, users_to_test)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Precision: {ret}')\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            joblib.dump(model, f'./model/NGCF_model_{epoch+1}.pkl')\n",
    "\n",
    "    \n",
    "    f = open(f'./data/rank.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        result_arr.append([*line.strip().split()])\n",
    "    f.close()\n",
    "    return result_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vhR4mD1uJx1"
   },
   "source": [
    "### 학습된 결과 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n5d2gZ8QuIgD",
    "outputId": "d8fac9fc-2b56-4909-fb79-3a75d1721e99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file exist\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get_result(epoch_value = 2000, flag = True)\n",
    "epoch_value = epoch 설정\n",
    "flag = 이미 저장된 파일 불러오려면 True, 새로 학습하려면 False\n",
    "'''\n",
    "\n",
    "user_problem_list = get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTHiXcbJ10dJ"
   },
   "source": [
    "## 결과 출력\n",
    "\n",
    "1. 유저가 이미 풀은 문제는 제외\n",
    "2. 유저 id와 문제 id가 매핑되어 있으므로 원래대로 바꾸어줌\n",
    "3. 유저가 풀지 않은 문제중에서 유사도가 제일 높은 30문제를 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pbOCKqMDuTYc"
   },
   "outputs": [],
   "source": [
    "def except_solved_problem(recommender_list):\n",
    "    problem_title_level_df = pd.read_csv(\"./data/userId_problemId.csv\").loc[:, ['problemId', 'title', 'level']]\n",
    "    user_problem_remap_df = pd.read_csv(\"./data/userId_problemId_remap.csv\").loc[:, ['userId', 'problemId']]\n",
    "    user_df = pd.read_csv(\"./data/user_list.csv\")\n",
    "    problem_df = pd.read_csv(\"./data/problem_list.csv\")\n",
    "\n",
    "    s = input(\"Search user name: \")\n",
    "\n",
    "    try:\n",
    "        user_id = int(user_df[user_df['user_id'] == s]['remap_id'])\n",
    "    except:\n",
    "        clear_output(wait=True)\n",
    "        print('cannot found user')\n",
    "        return []\n",
    "\n",
    "\n",
    "    user_recommender_list = recommender_list[user_id]\n",
    "    search_user_df = user_problem_remap_df[user_problem_remap_df['userId'] == user_id]\n",
    "    \n",
    "    # remap_id → boj 문제 번호\n",
    "    boj_problem_list = []\n",
    "    for rec_id in user_recommender_list:\n",
    "        if search_user_df[search_user_df['problemId'] == float(rec_id)].size == 0:\n",
    "            boj_problem_list.append(int(problem_df[problem_df['remap_id'] == int(rec_id)]['problem_id']))\n",
    "    \n",
    "    # boj 문제 번호 -> search level, title\n",
    "\n",
    "    res_boj_info = []\n",
    "    for boj_problem_id in boj_problem_list:\n",
    "        df = problem_title_level_df[problem_title_level_df['problemId'] == boj_problem_id].iloc[0]\n",
    "        if 10 < df['level']:\n",
    "            res_boj_info.append([df['problemId'], df['title'], df['level']])\n",
    "\n",
    "\n",
    "    return res_boj_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDewrOLQNAIi",
    "outputId": "dbaba295-84e8-4329-aa1f-16129514e308",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search user name: alstjr3060\n"
     ]
    }
   ],
   "source": [
    "res_arr = except_solved_problem(user_problem_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32IV_S7RaCec",
    "outputId": "8fd04feb-23e9-446d-b6ad-7a61dcf0af16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOJ 1918\n",
      "문제 이름: 후위 표기식\n",
      "문제 난이도: Gold 3\n",
      "문제 유형: data_structures, stack\n",
      "\n",
      "BOJ 3079\n",
      "문제 이름: 입국심사\n",
      "문제 난이도: Gold 1\n",
      "문제 유형: binary_search, parametric_search\n",
      "\n",
      "BOJ 11779\n",
      "문제 이름: 최소비용 구하기 2\n",
      "문제 난이도: Gold 3\n",
      "문제 유형: dijkstra, graphs\n",
      "\n",
      "BOJ 1707\n",
      "문제 이름: 이분 그래프\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: bfs, dfs, graph_traversal, graphs\n",
      "\n",
      "BOJ 18405\n",
      "문제 이름: 경쟁적 전염\n",
      "문제 난이도: Gold 1\n",
      "문제 유형: bfs, graph_traversal, graphs, implementation, simulation\n",
      "\n",
      "BOJ 9370\n",
      "문제 이름: 미확인 도착지\n",
      "문제 난이도: Gold 4\n",
      "문제 유형: dijkstra, graphs\n",
      "\n",
      "BOJ 1987\n",
      "문제 이름: 알파벳\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: backtracking, dfs, graph_traversal, graphs\n",
      "\n",
      "BOJ 16724\n",
      "문제 이름: 피리 부는 사나이\n",
      "문제 난이도: Gold 4\n",
      "문제 유형: data_structures, dfs, disjoint_set, graph_traversal, graphs\n",
      "\n",
      "BOJ 1520\n",
      "문제 이름: 내리막 길\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: dfs, dp, graph_traversal, graphs\n",
      "\n",
      "BOJ 14925\n",
      "문제 이름: 목장 건설하기\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: dp\n",
      "\n",
      "BOJ 3584\n",
      "문제 이름: 가장 가까운 공통 조상\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: graph_traversal, graphs, lca, trees\n",
      "\n",
      "BOJ 9466\n",
      "문제 이름: 텀 프로젝트\n",
      "문제 난이도: Gold 3\n",
      "문제 유형: dfs, graph_traversal, graphs\n",
      "\n",
      "BOJ 2665\n",
      "문제 이름: 미로만들기\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: bfs, dijkstra, graph_traversal, graphs\n",
      "\n",
      "BOJ 9935\n",
      "문제 이름: 문자열 폭발\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: data_structures, stack, string\n",
      "\n",
      "BOJ 17404\n",
      "문제 이름: RGB거리 2\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: dp\n",
      "\n",
      "BOJ 2239\n",
      "문제 이름: 스도쿠\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: backtracking, implementation\n",
      "\n",
      "BOJ 1719\n",
      "문제 이름: 택배\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: dijkstra, floyd_warshall, graphs\n",
      "\n",
      "BOJ 17142\n",
      "문제 이름: 연구소 3\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: bfs, bruteforcing, graph_traversal, graphs\n",
      "\n",
      "BOJ 14938\n",
      "문제 이름: 서강그라운드\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: dijkstra, floyd_warshall, graphs\n",
      "\n",
      "BOJ 3745\n",
      "문제 이름: 오름세\n",
      "문제 난이도: Gold 4\n",
      "문제 유형: binary_search, lis\n",
      "\n",
      "BOJ 14921\n",
      "문제 이름: 용액 합성하기\n",
      "문제 난이도: Gold 1\n",
      "문제 유형: sorting, two_pointer\n",
      "\n",
      "BOJ 16933\n",
      "문제 이름: 벽 부수고 이동하기 3\n",
      "문제 난이도: Gold 5\n",
      "문제 유형: bfs, graph_traversal, graphs\n",
      "\n",
      "BOJ 2146\n",
      "문제 이름: 다리 만들기\n",
      "문제 난이도: Gold 3\n",
      "문제 유형: bfs, graph_traversal, graphs\n",
      "\n",
      "BOJ 2211\n",
      "문제 이름: 네트워크 복구\n",
      "문제 난이도: Gold 4\n",
      "문제 유형: dijkstra, graphs\n",
      "\n",
      "BOJ 20166\n",
      "문제 이름: 문자열 지옥에 빠진 호석\n",
      "문제 난이도: Gold 1\n",
      "문제 유형: data_structures, dfs, graph_traversal, graphs, hash_set, string\n",
      "\n",
      "BOJ 17472\n",
      "문제 이름: 다리 만들기 2\n",
      "문제 난이도: Gold 5\n",
      "문제 유형: bfs, bruteforcing, dfs, graph_traversal, graphs, implementation, mst\n",
      "\n",
      "BOJ 16946\n",
      "문제 이름: 벽 부수고 이동하기 4\n",
      "문제 난이도: Gold 4\n",
      "문제 유형: bfs, dfs, graph_traversal, graphs\n",
      "\n",
      "BOJ 11049\n",
      "문제 이름: 행렬 곱셈 순서\n",
      "문제 난이도: Gold 3\n",
      "문제 유형: dp\n",
      "\n",
      "BOJ 4256\n",
      "문제 이름: 트리\n",
      "문제 난이도: Gold 3\n",
      "문제 유형: divide_and_conquer, recursion, trees\n",
      "\n",
      "BOJ 12869\n",
      "문제 이름: 뮤탈리스크\n",
      "문제 난이도: Gold 2\n",
      "문제 유형: dp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tier = ['Bronze', 'Silver', 'Gold']\n",
    "tier_list = ['Unrank']\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        tier_list.append(f'{tier[i]} {j+1}')\n",
    "\n",
    "problem_tag_df = pd.read_csv(\"./data/problem_category.csv\").loc[:, ['problemId', 'category']]\n",
    "for i in range(30):\n",
    "    val, title, level = res_arr[i]\n",
    "    tag = problem_tag_df[problem_tag_df['problemId'] == val]['category'].tolist()\n",
    "    print(f'BOJ {val}\\n문제 이름: {title}\\n문제 난이도: {tier_list[level]}\\n문제 유형: {\", \".join(tag)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정확도 측정\n",
    "1. 각 유저마다 유사도가 높은 100문제를 가져옴\n",
    "2. 유사도가 높은 100개의 문제중에 이미 풀은 문제가 몇개인지 확인\n",
    "3. 평균값을 구해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sol_precision():\n",
    "    user_problem_remap_df = pd.read_csv(\"./data/userId_problemId_remap.csv\").loc[:, ['userId', 'problemId']]\n",
    "    \n",
    "    result = []\n",
    "    for i in range(2000):\n",
    "        if (i+1) % 50 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print('Loading: [{}]'.format('■' * ((i+1) // 50) + '□' * (40 - (i+1) // 50)))\n",
    "        \n",
    "        each_user_recommender_list = user_problem_list[i][:100]\n",
    "        each_user_problem_remap_list = user_problem_remap_df[user_problem_remap_df['userId'] == i]['problemId'].tolist()\n",
    "        correct = 0\n",
    "        \n",
    "        for x in each_user_recommender_list:\n",
    "            for y in each_user_problem_remap_list:\n",
    "                if int(x) == y:\n",
    "                    correct += 1\n",
    "        \n",
    "        result.append(correct)\n",
    "    \n",
    "    return sum(result) / 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: [■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■]\n"
     ]
    }
   ],
   "source": [
    "precision = sol_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 44.70%\n"
     ]
    }
   ],
   "source": [
    "print(f'정확도: {precision*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
